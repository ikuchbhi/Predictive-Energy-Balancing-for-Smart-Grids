{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Enhanced REDf (Recurrent Energy Demand forecasting) Model Implementation\n",
                "\n",
                "This notebook implements an enhanced version of the REDf model for energy demand forecasting. The implementation uses a hybrid CNN-LSTM architecture with Temporal Pattern Attention (TPA) mechanism and includes hyperparameter tuning to optimize model performance. The model processes hourly energy consumption data from multiple regions to predict future energy demand."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-04-18 06:30:00.546986: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
                        "2025-04-18 06:30:00.550372: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
                        "2025-04-18 06:30:00.559219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
                        "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
                        "E0000 00:00:1744938000.574969  124052 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
                        "E0000 00:00:1744938000.578839  124052 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "W0000 00:00:1744938000.590488  124052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
                        "W0000 00:00:1744938000.590514  124052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
                        "W0000 00:00:1744938000.590516  124052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
                        "W0000 00:00:1744938000.590517  124052 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
                        "2025-04-18 06:30:00.594176: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "from tensorflow.keras.models import Sequential, Model\n",
                "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Conv1D, MaxPooling1D, Multiply, GlobalAveragePooling1D\n",
                "import tensorflow as tf\n",
                "import os\n",
                "import requests\n",
                "import warnings\n",
                "from tensorflow.keras.callbacks import EarlyStopping\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "# Set random seed for reproducibility\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Download and Preparation\n",
                "\n",
                "This function downloads the required energy consumption datasets from GitHub if they don't already exist locally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "AEP dataset already exists.\n",
                        "COMED dataset already exists.\n",
                        "DAYTON dataset already exists.\n",
                        "PJME dataset already exists.\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "os.makedirs(\"data\", exist_ok=True)\n",
                "dataset_urls = {\n",
                "    \"AEP\": \"https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/refs/heads/master/data/AEP_hourly.csv\",\n",
                "    \"COMED\": \"https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/refs/heads/master/data/COMED_hourly.csv\",\n",
                "    \"DAYTON\": \"https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/refs/heads/master/data/DAYTON_hourly.csv\",\n",
                "    \"PJME\": \"https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/refs/heads/master/data/PJME_hourly.csv\",\n",
                "}\n",
                "\n",
                "dataset_paths = {}\n",
                "for name, url in dataset_urls.items():\n",
                "    file_path = f\"data/{name}_hourly.csv\"\n",
                "    dataset_paths[name] = file_path\n",
                "    if not os.path.exists(file_path):\n",
                "        print(f\"Downloading {name} dataset...\")\n",
                "        try:\n",
                "            response = requests.get(url)\n",
                "            response.raise_for_status()\n",
                "            with open(file_path, \"w\") as f:\n",
                "                f.write(response.text)\n",
                "            print(f\"{name} dataset downloaded successfully.\")\n",
                "        except Exception as e:\n",
                "            print(f\"Error downloading {name} dataset: {e}\")\n",
                "    else:\n",
                "        print(f\"{name} dataset already exists.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Algorithm 1: Data Pre-processing\n",
                "\n",
                "Transform raw data into pre-processed data suitable for model training and evaluation.\n",
                "\n",
                "### Steps:\n",
                "1. **Input**: Raw Data `Draw`\n",
                "2. **Output**: Pre-processed Data `Dpre`\n",
                "3. **Procedure**:\n",
                "   - **Step 4**: Load the raw data into the program: `Draw ← load_data()`\n",
                "   - **Step 5**: Check for missing values and handle them accordingly: `Dpre ← handle_missing_values(Draw)`\n",
                "   - **Step 6**: Check for outliers and handle them accordingly: `Dpre ← handle_outliers(Dpre)`\n",
                "   - **Step 7**: Normalize the data: `Dpre ← normalize(Dpre)`\n",
                "   - **Step 8**: Divide the data into training and testing sets: `(Dtrain, Dtest) ← split_data(Dpre)`\n",
                "   - **Step 9**: Return the pre-processed data: `return Dpre`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_data(file_path):\n",
                "\n",
                "    data = pd.read_csv(file_path)\n",
                "\n",
                "    data[\"Datetime\"] = pd.to_datetime(data[\"Datetime\"])\n",
                "    data.set_index(\"Datetime\", inplace=True)\n",
                "    \n",
                "    energy_column = data.columns[0]\n",
                "    energy_demand = data[energy_column].values.reshape(-1, 1)\n",
                "\n",
                "\n",
                "    z_scores = (energy_demand - np.mean(energy_demand)) / np.std(energy_demand)\n",
                "    abs_z_scores = np.abs(z_scores)\n",
                "    outlier_indices = (abs_z_scores > 3).flatten()\n",
                "\n",
                "\n",
                "    median_value = np.median(energy_demand)\n",
                "    energy_demand[outlier_indices] = median_value\n",
                "\n",
                "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
                "    scaled_data = scaler.fit_transform(energy_demand)\n",
                "\n",
                "    return scaled_data, scaler, energy_column"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sequence Creation for Time Series\n",
                "\n",
                "This section describes the function responsible for generating input-output sequences for time series forecasting. The model requires input sequences of several hours (24 in this case) to predict future energy demand accurately."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_sequences(data, time_steps):\n",
                "    X, y = [], []\n",
                "    for i in range(len(data) - time_steps):\n",
                "        X.append(data[i : (i + time_steps), 0])\n",
                "        y.append(data[i + time_steps, 0])\n",
                "    return np.array(X), np.array(y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Enhanced REDf Model Architecture: Hybrid CNN-LSTM with Temporal Pattern Attention\n",
                "\n",
                "This section introduces an enhanced architecture for the REDf model that incorporates CNN layers for feature extraction, multiple stacked LSTM layers for temporal processing, and a Temporal Pattern Attention (TPA) mechanism.\n",
                "\n",
                "### Key Components of the Enhanced Model:\n",
                "\n",
                "1. **CNN Feature Extraction Block**:\n",
                "   - Uses 1D convolutional layers to extract local patterns from the time series data\n",
                "   - Applies max pooling to reduce dimensionality and focus on important features\n",
                "\n",
                "2. **Multi-layer LSTM Block**:\n",
                "   - Uses three stacked LSTM layers to capture complex temporal dependencies\n",
                "   - Each LSTM layer is followed by a dropout layer to prevent overfitting\n",
                "\n",
                "3. **Temporal Pattern Attention Mechanism**:\n",
                "   - Applies attention weights to focus on the most relevant temporal patterns\n",
                "   - Uses convolutional features to generate attention scores\n",
                "   - Weights the LSTM outputs according to their importance for prediction\n",
                "\n",
                "4. **Dense Output Layer**:\n",
                "   - Produces the final energy demand prediction\n",
                "\n",
                "This architecture enhances the original REDf model by incorporating feature extraction capabilities and attention mechanisms that can better capture complex patterns in energy demand data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_redf_model(input_shape, lstm_units=250, dropout_rate=0.1):\n",
                "    inputs = Input(shape=(input_shape, 1))\n",
                "    \n",
                "    # CNN Feature Extraction Block\n",
                "    x = Conv1D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\")(inputs)\n",
                "    x = MaxPooling1D(pool_size=2)(x)\n",
                "    \n",
                "    # LSTM Temporal Processing Block with multiple LSTM layers\n",
                "    x = LSTM(lstm_units, return_sequences=True)(x)\n",
                "    x = Dropout(dropout_rate)(x)\n",
                "    x = LSTM(lstm_units, return_sequences=True)(x)\n",
                "    x = Dropout(dropout_rate)(x)\n",
                "    x = LSTM(lstm_units, return_sequences=True)(x)\n",
                "    x = Dropout(dropout_rate)(x)\n",
                "    \n",
                "    # Temporal Pattern Attention Mechanism\n",
                "    conv_features = Conv1D(64, 3, padding=\"same\", activation=\"tanh\")(x)\n",
                "    attention_scores = Dense(1, activation=\"sigmoid\")(conv_features)\n",
                "    weighted_features = Multiply()([x, attention_scores])\n",
                "    context_vector = GlobalAveragePooling1D()(weighted_features)\n",
                "    \n",
                "    outputs = Dense(1)(context_vector)\n",
                "    \n",
                "    model = Model(inputs=inputs, outputs=outputs)\n",
                "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hyperparameter Tuning with Grid Search\n",
                "\n",
                "This section implements a grid search approach to find the optimal hyperparameters for the model. Unlike the original implementation which used fixed hyperparameters, this enhanced version explores different combinations to identify the best configuration for each dataset.\n",
                "\n",
                "### Grid Search Parameters:\n",
                "\n",
                "- **LSTM Units**: Number of units in each LSTM layer (options: 200, 250, 300)\n",
                "- **Dropout Rate**: Rate of dropout for regularization (options: 0.1, 0.2)\n",
                "- **Batch Size**: Number of samples per gradient update (options: 500, 1000, 1500)\n",
                "- **Epochs**: Number of complete passes through the training dataset (options: 30, 50)\n",
                "\n",
                "The function performs an exhaustive search over all combinations of these parameters and selects the best based on validation loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def grid_search_hyperparameters(X_train, y_train, X_val, y_val, input_shape):\n",
                "    # Define parameter ranges\n",
                "    lstm_units_options = [200, 250]\n",
                "    dropout_rate_options = [0.1, 0.2]\n",
                "    batch_size_options = [500]\n",
                "    epoch_options = [30, 40,50]\n",
                "\n",
                "    best_val_loss = np.inf\n",
                "    best_params = {}\n",
                "    best_history = None\n",
                "\n",
                "    for lstm_units in lstm_units_options:\n",
                "        for dropout_rate in dropout_rate_options:\n",
                "            for batch_size in batch_size_options:\n",
                "                for epochs in epoch_options:\n",
                "                    print(\n",
                "                        f\"Testing params: LSTM Units={lstm_units}, Dropout={dropout_rate}, Batch Size={batch_size}, Epochs={epochs}\"\n",
                "                    )\n",
                "                    model = build_redf_model(\n",
                "                        input_shape, lstm_units=lstm_units, dropout_rate=dropout_rate\n",
                "                    )\n",
                "\n",
                "                    early_stopping = EarlyStopping(\n",
                "                        monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
                "                    )\n",
                "\n",
                "                    history = model.fit(\n",
                "                        X_train,\n",
                "                        y_train,\n",
                "                        epochs=epochs,\n",
                "                        batch_size=batch_size,\n",
                "                        validation_data=(X_val, y_val),\n",
                "                        verbose=0,\n",
                "                        callbacks=[early_stopping],\n",
                "                    )\n",
                "                    val_loss = min(history.history[\"val_loss\"])\n",
                "                    print(f\"Validation loss: {val_loss:.4f}\")\n",
                "\n",
                "                    if val_loss < best_val_loss:\n",
                "                        best_val_loss = val_loss\n",
                "                        best_params = {\n",
                "                            \"lstm_units\": lstm_units,\n",
                "                            \"dropout_rate\": dropout_rate,\n",
                "                            \"batch_size\": batch_size,\n",
                "                            \"epochs\": epochs,\n",
                "                        }\n",
                "                        best_history = history\n",
                "\n",
                "    print(\"Best Hyperparameters:\", best_params)\n",
                "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
                "    return best_params, best_history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training and Evaluation Process\n",
                "\n",
                "This function trains the enhanced REDf model with the optimal hyperparameters identified during the grid search and evaluates its performance on the test set.\n",
                "\n",
                "### Key Components:\n",
                "- Early stopping to prevent overfitting\n",
                "- Evaluation using multiple metrics (MAE, RMSE, R²)\n",
                "- Detailed reporting of model performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_redf(\n",
                "    X_train, y_train, X_val, y_val, epochs, batch_size, lstm_units, dropout_rate\n",
                "):\n",
                "    \n",
                "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
                "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
                "    \n",
                "    model = build_redf_model(\n",
                "        X_train.shape[1], lstm_units=lstm_units, dropout_rate=dropout_rate\n",
                "    )\n",
                "    \n",
                "    model.summary()\n",
                "    \n",
                "    early_stopping = EarlyStopping(\n",
                "        monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
                "    )\n",
                "    \n",
                "    history = model.fit(\n",
                "        X_train,\n",
                "        y_train,\n",
                "        epochs=epochs,\n",
                "        batch_size=batch_size,\n",
                "        validation_data=(X_val, y_val),\n",
                "        verbose=1,\n",
                "        callbacks=[early_stopping],\n",
                "    )\n",
                "    \n",
                "    \n",
                "    return model, history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Main Workflow\n",
                "\n",
                "This section orchestrates the complete workflow for the enhanced REDf model implementation:\n",
                "\n",
                "1. **Preprocess Data**: For each dataset, preprocess the raw data by handling missing values, removing outliers, and normalizing the energy demand values.\n",
                "2. **Create Sequences**: Generate input-output sequences for time series forecasting using a sliding window approach.\n",
                "3. **Split Data**: Divide the sequences into training (80%) and testing (20%) sets.\n",
                "4. **Hyperparameter Tuning**: Find optimal hyperparameters\n",
                "5. **Train the Model**: Train the REDf model using the training data and evaluate its performance on the testing data.\n",
                "6. **Save the Model**: Save the trained model for each dataset for future use.\n",
                "\n",
                "This workflow ensures a systematic approach to model training and evaluation, enabling reproducibility and consistent performance assessment across datasets.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "time_steps = 24  # 24 hours (1 day) sequence length\n",
                "best_hyperparams = {}\n",
                "\n",
                "for name, path in dataset_paths.items():\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Processing {name} dataset...\")\n",
                "    print(f\"{'='*50}\")\n",
                "    \n",
                "    try:\n",
                "        scaled_data, scaler, energy_column = preprocess_data(path)\n",
                "        \n",
                "        X, y = create_sequences(scaled_data, time_steps)\n",
                "        \n",
                "        total_samples = len(X)\n",
                "        train_end = int(total_samples * 0.8)\n",
                "        val_end = int(total_samples * 0.9)\n",
                "        \n",
                "        X_train, y_train = X[:train_end], y[:train_end]\n",
                "        X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
                "        X_test, y_test = X[val_end:], y[val_end:]\n",
                "        \n",
                "        print(f\"Training set shape: {X_train.shape}\")\n",
                "        print(f\"Validation set shape: {X_val.shape}\")\n",
                "        print(f\"Testing set shape: {X_test.shape}\")\n",
                "        \n",
                "        best_params, _ = grid_search_hyperparameters(\n",
                "            X_train,\n",
                "            y_train,\n",
                "            X_val,\n",
                "            y_val,\n",
                "            input_shape=X_train.shape[1],\n",
                "        )\n",
                "        \n",
                "        best_hyperparams[name] = best_params\n",
                "        \n",
                "        model, history = train_redf(\n",
                "            X_train,\n",
                "            y_train,\n",
                "            X_val,\n",
                "            y_val,\n",
                "            epochs=best_params[\"epochs\"],\n",
                "            batch_size=best_params[\"batch_size\"],\n",
                "            lstm_units=best_params[\"lstm_units\"],\n",
                "            dropout_rate=best_params[\"dropout_rate\"],\n",
                "        )\n",
                "        \n",
                "        # Save the model\n",
                "        model.save(f\"{name}.h5\")\n",
                "        print(f\"Model saved as {name}.h5\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error processing {name} dataset: {e}\")\n",
                "\n",
                "\n",
                "print(\"\\nBest Hyperparameters per Dataset:\")\n",
                "for name, params in best_hyperparams.items():\n",
                "    print(f\"{name}: {params}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Loading and evaluation\n",
                "\n",
                "1. **Load the Model**: The saved model can be loaded using the `load_model()` function from Keras.\n",
                "2. **Inference**: The loaded model is used to make predictions on new or test data.\n",
                "3. **Evaluation**: The predictions are evaluated using metrics such as MAE, RMSE, and R^2 to assess the model\\'s performance.\n",
                "\n",
                "\n",
                "This implementation provides several improvements over the original REDf model:\n",
                "\n",
                "1. **Enhanced Architecture**: Hybrid CNN-LSTM with Temporal Pattern Attention\n",
                "2. **Hyperparameter Tuning**: Systematic grid search for optimal parameters\n",
                "3. **Improved Regularization**: Multiple dropout layers and early stopping\n",
                "4. **Better Data Utilization**: Separate validation set for hyperparameter selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==================================================\n",
                        "Processing AEP dataset...\n",
                        "==================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-04-18 06:30:18.200453: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
                        "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step\n",
                        "\n",
                        "==================================================\n",
                        "Processing COMED dataset...\n",
                        "==================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step\n",
                        "\n",
                        "==================================================\n",
                        "Processing DAYTON dataset...\n",
                        "==================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step\n",
                        "\n",
                        "==================================================\n",
                        "Processing PJME dataset...\n",
                        "==================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1m455/455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step\n"
                    ]
                }
            ],
            "source": [
                "from tensorflow.keras.models import load_model\n",
                "import glob\n",
                "\n",
                "# Load all models from the ./models_new folder\n",
                "model_files = glob.glob(\"./models_new/*.h5\")\n",
                "loaded_models = {}\n",
                "\n",
                "for model_file in model_files:\n",
                "    model_name = model_file.split(\"/\")[-1].replace(\".h5\", \"\")\n",
                "    loaded_models[model_name] = model_file\n",
                "\n",
                "results = {}\n",
                "for name, path in dataset_paths.items():\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Processing {name} dataset...\")\n",
                "    print(f\"{'='*50}\")\n",
                "    \n",
                "    \n",
                "    scaled_data, scaler, energy_column = preprocess_data(path)\n",
                "    \n",
                "    time_steps = 24\n",
                "    X, y = create_sequences(scaled_data, time_steps)\n",
                "    \n",
                "    total_samples = len(X)\n",
                "    train_end = int(total_samples * 0.8)\n",
                "    val_end = int(total_samples * 0.9)\n",
                "    \n",
                "    X_train, y_train = X[:train_end], y[:train_end]\n",
                "    X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
                "    X_test, y_test = X[val_end:], y[val_end:]\n",
                "\n",
                "    model = load_model(loaded_models[name], custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
                "\n",
                "    \n",
                "    y_pred = model.predict(X_test)\n",
                "    \n",
                "    mae = mean_absolute_error(y_test, y_pred)\n",
                "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
                "    r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "    results[name] = (mae, rmse, r2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Final Output\n",
                "\n",
                "This section summarizes the performance of the REDf model on each dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Summary of Results:\n",
                        "------------------\n",
                        "Dataset | MAE | RMSE | R²\n",
                        "---------------------------------\n",
                        "AEP     | 0.0143 | 0.0222 | 0.9848\n",
                        "COMED   | 0.0213 | 0.0414 | 0.9432\n",
                        "DAYTON  | 0.0151 | 0.0238 | 0.9797\n",
                        "PJME    | 0.0131 | 0.0254 | 0.9750\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "print(\"Dataset | MAE | RMSE | R²\")\n",
                "print(\"---------------------------------\")\n",
                "for name, metrics in results.items():\n",
                "    mae, rmse, r2 = metrics\n",
                "    print(f\"{name.ljust(7)} | {mae:.4f} | {rmse:.4f} | {r2:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "myenv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
