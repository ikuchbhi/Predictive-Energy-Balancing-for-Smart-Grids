{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDf (Recurrent Energy Demand forecasting) Model Implementation\n",
    "\n",
    "This notebook implements the REDf model for energy demand forecasting using LSTM neural networks. The model processes hourly energy consumption data from multiple regions to predict future energy demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 22:08:06.106014: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-17 22:08:06.110039: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-17 22:08:06.118303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744907886.133020   77932 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744907886.137011   77932 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744907886.148767   77932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744907886.148790   77932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744907886.148792   77932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744907886.148794   77932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-17 22:08:06.152617: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download and Preparation\n",
    "\n",
    "This function downloads the required energy consumption datasets from GitHub if they don't already exist locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEP dataset already exists.\n",
      "COMED dataset already exists.\n",
      "DAYTON dataset already exists.\n",
      "PJME dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "dataset_urls = {\n",
    "    \"AEP\": \"https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/refs/heads/master/data/AEP_hourly.csv\",\n",
    "    \"COMED\": \"https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/refs/heads/master/data/COMED_hourly.csv\",\n",
    "    \"DAYTON\": \"https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/refs/heads/master/data/DAYTON_hourly.csv\",\n",
    "    \"PJME\": \"https://raw.githubusercontent.com/panambY/Hourly_Energy_Consumption/refs/heads/master/data/PJME_hourly.csv\",\n",
    "}\n",
    "\n",
    "dataset_paths = {}\n",
    "for name, url in dataset_urls.items():\n",
    "    file_path = f\"data/{name}_hourly.csv\"\n",
    "    dataset_paths[name] = file_path\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading {name} dataset...\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"{name} dataset downloaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {name} dataset: {e}\")\n",
    "    else:\n",
    "        print(f\"{name} dataset already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Algorithm 1: Data Pre-processing\n",
    "\n",
    "Transform raw data into pre-processed data suitable for model training and evaluation.\n",
    "\n",
    "### Steps:\n",
    "1. **Input**: Raw Data `Draw`\n",
    "2. **Output**: Pre-processed Data `Dpre`\n",
    "3. **Procedure**:\n",
    "    - **Step 4**: Load the raw data into the program: `Draw ← load_data()`\n",
    "    - **Step 5**: Check for missing values and handle them accordingly: `Dpre ← handle_missing_values(Draw)`\n",
    "    - **Step 6**: Check for outliers and handle them accordingly: `Dpre ← handle_outliers(Dpre)`\n",
    "    - **Step 7**: Normalize the data: `Dpre ← normalize(Dpre)`\n",
    "    - **Step 8**: Divide the data into training and testing sets: `(Dtrain, Dtest) ← split_data(Dpre)`\n",
    "    - **Step 9**: Return the pre-processed data: `return Dpre`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path):\n",
    "\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    data[\"Datetime\"] = pd.to_datetime(data[\"Datetime\"])\n",
    "    data.set_index(\"Datetime\", inplace=True)\n",
    "    \n",
    "    energy_column = data.columns[0]\n",
    "    energy_demand = data[energy_column].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    z_scores = (energy_demand - np.mean(energy_demand)) / np.std(energy_demand)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    outlier_indices = (abs_z_scores > 3).flatten()\n",
    "\n",
    "\n",
    "    median_value = np.median(energy_demand)\n",
    "    energy_demand[outlier_indices] = median_value\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(energy_demand)\n",
    "\n",
    "    return scaled_data, scaler, energy_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Creation for Time Series\n",
    "\n",
    "This section describes the function responsible for generating input-output sequences for time series forecasting. The model requires input sequences of several hours (24 in this case) to predict future energy demand accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, time_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i : (i + time_steps), 0])\n",
    "        y.append(data[i + time_steps, 0])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDf Model Architecture\n",
    "\n",
    "This section outlines the REDf model architecture as described in Algorithm 3 of the referenced paper.\n",
    "\n",
    "### Algorithm 3: Proposed Forecasting Model\n",
    "\n",
    "1. **Initialize the model**: `model = Sequential()`\n",
    "2. **Add an LSTM layer with 200 units**: `model.add(LSTM(200, input_shape=(timesteps, features)))`\n",
    "3. **Add a dropout layer with a rate of 0.1**: `model.add(Dropout(0.1))`\n",
    "4. **Add another LSTM layer with 200 units**: `model.add(LSTM(200, return_sequences=False))`\n",
    "5. **Add another dropout layer with a rate of 0.1**: `model.add(Dropout(0.1))`\n",
    "6. **Add a fully connected layer with 1 unit**: `model.add(Dense(1))`\n",
    "7. **Compile the model**: `model.compile(loss='mse', optimizer='adam')`\n",
    "8. **Train the model on the training data**: `model.fit(X_train, y_train, epochs=10, batch_size=1000)`\n",
    "9. **Generate predictions on the test data**: `y_pred = model.predict(X_test)`\n",
    "\n",
    "The implementation uses the hyperparameters specified in the paper after grid search, including:\n",
    "- LSTM units: 200\n",
    "- Dropout rate: 0.1\n",
    "- Epochs: 10\n",
    "- Batch size: 1000\n",
    "\n",
    "This approach skips grid search for hyperparameter tuning, as the final hyperparameters are directly adopted from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_redf_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, return_sequences=True, input_shape=(input_shape, 1)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(200, return_sequences=False))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_redf(X_train, y_train, X_test, y_test, epochs=10, batch_size=1000):\n",
    "    # Reshape input for LSTM [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    model = build_redf_model(X_train.shape[1])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training Workflow\n",
    "\n",
    "This section outlines the workflow for training and evaluating the REDf model on multiple datasets. The process involves the following steps:\n",
    "\n",
    "1. **Set Parameters**: Define key hyperparameters such as sequence length (`time_steps`), number of epochs, and batch size.\n",
    "2. **Preprocess Data**: For each dataset, preprocess the raw data by handling missing values, removing outliers, and normalizing the energy demand values.\n",
    "3. **Create Sequences**: Generate input-output sequences for time series forecasting using a sliding window approach.\n",
    "4. **Split Data**: Divide the sequences into training (80%) and testing (20%) sets.\n",
    "5. **Train the Model**: Train the REDf model using the training data and evaluate its performance on the testing data.\n",
    "6. **Save the Model**: Save the trained model for each dataset for future use.\n",
    "\n",
    "This workflow ensures a systematic approach to model training and evaluation, enabling reproducibility and consistent performance assessment across datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set parameters as specified in the paper\n",
    "time_steps = 24  \n",
    "epochs = 10     \n",
    "batch_size = 1000  \n",
    "\n",
    "for name, path in dataset_paths.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {name} dataset...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        scaled_data, scaler, energy_column = preprocess_data(path)\n",
    "        \n",
    "        X, y = create_sequences(scaled_data, time_steps)\n",
    "        \n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "        \n",
    "        print(f\"Training set shape: {X_train.shape}\")\n",
    "        print(f\"Testing set shape: {X_test.shape}\")\n",
    "        \n",
    "        model, history = train_redf(\n",
    "            X_train, y_train, X_test, y_test, epochs=epochs, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Save the model\n",
    "        model.save(f\"{name}.h5\")\n",
    "        print(f\"Model saved as {name}.h5\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {name} dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and evaluation\n",
    "\n",
    "1. **Load the Model**: The saved model can be loaded using the `load_model()` function from Keras.\n",
    "2. **Inference**: The loaded model is used to make predictions on new or test data.\n",
    "3. **Evaluation**: The predictions are evaluated using metrics such as MAE, RMSE, and R^2 to assess the model\\'s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing AEP dataset...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 22:08:20.803055: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m758/758\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing COMED dataset...\n",
      "==================================================\n",
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step\n",
      "\n",
      "==================================================\n",
      "Processing DAYTON dataset...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m758/758\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 12ms/step\n",
      "\n",
      "==================================================\n",
      "Processing PJME dataset...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m909/909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import glob\n",
    "\n",
    "# Load all models from the ./models folder\n",
    "model_files = glob.glob(\"./models/*.h5\")\n",
    "loaded_models = {}\n",
    "\n",
    "for model_file in model_files:\n",
    "    model_name = model_file.split(\"/\")[-1].replace(\".h5\", \"\")\n",
    "    loaded_models[model_name] = model_file\n",
    "\n",
    "results = {}\n",
    "for name, path in dataset_paths.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {name} dataset...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    \n",
    "    scaled_data, scaler, energy_column = preprocess_data(path)\n",
    "    \n",
    "    time_steps = 24\n",
    "    X, y = create_sequences(scaled_data, time_steps)\n",
    "    \n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    model = load_model(loaded_models[name], custom_objects={\"mse\": tf.keras.losses.MeanSquaredError()})\n",
    "\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    results[name] = (mae, rmse, r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Output\n",
    "\n",
    "This section summarizes the performance of the REDf model on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Results:\n",
      "------------------\n",
      "Dataset | MAE | RMSE | R²\n",
      "---------------------------------\n",
      "AEP     | 0.0229 | 0.0337 | 0.9676\n",
      "COMED   | 0.0347 | 0.0550 | 0.9118\n",
      "DAYTON  | 0.0225 | 0.0358 | 0.9552\n",
      "PJME    | 0.0241 | 0.0400 | 0.9450\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset | MAE | RMSE | R²\")\n",
    "print(\"---------------------------------\")\n",
    "for name, metrics in results.items():\n",
    "    mae, rmse, r2 = metrics\n",
    "    print(f\"{name.ljust(7)} | {mae:.4f} | {rmse:.4f} | {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
